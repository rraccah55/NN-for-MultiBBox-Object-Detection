{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "initial_id",
      "metadata": {
        "collapsed": true,
        "ExecuteTime": {
          "end_time": "2024-03-09T12:57:48.843566Z",
          "start_time": "2024-03-09T12:57:48.837149Z"
        },
        "id": "initial_id"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from typing import Optional\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.models as models\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from torchvision import transforms\n",
        "import albumentations\n",
        "from torch.utils.data import DataLoader\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vzTA5TfjeWts",
        "outputId": "dcdb683d-96e8-448b-bfc4-66f7e86c97eb"
      },
      "id": "vzTA5TfjeWts",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "outputs": [],
      "source": [
        "PATH = '/content/drive/MyDrive/Studies/2024/Image Processing/NN Project/MultipleBBox/'\n",
        "VIDEO_PATH = PATH + \"video.mp4\"\n",
        "RESULT_VIDEO_PATH = PATH + \"result_video.mp4\"\n",
        "CHECKPOINT_FILE = PATH + 'checkpoints/checkpoint_itzik_7.pt'\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "IMAGE_HEIGHT, IMAGE_WIDTH = 224, 224\n",
        "\n",
        "INT_TO_LABEL = {\n",
        "    0: \"aeroplane\",\n",
        "    1: \"bicycle\",\n",
        "    2: \"bird\",\n",
        "    3: \"boat\",\n",
        "    4: \"bottle\",\n",
        "    5: \"bus\",\n",
        "    6: \"car\",\n",
        "    7: \"cat\",\n",
        "    8: \"chair\",\n",
        "    9: \"cow\",\n",
        "    10: \"dining table\",\n",
        "    11: \"dog\",\n",
        "    12: \"horse\",\n",
        "    13: \"motorbike\",\n",
        "    14: \"person\",\n",
        "    15: \"potted plant\",\n",
        "    16: \"sheep\",\n",
        "    17: \"sofa\",\n",
        "    18: \"train\",\n",
        "    19: \"tv/monitor\"\n",
        "}"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-03-09T12:10:47.047416Z",
          "start_time": "2024-03-09T12:10:47.040730Z"
        },
        "id": "b04b0b1e9d82ab11"
      },
      "id": "b04b0b1e9d82ab11",
      "execution_count": 3
    },
    {
      "cell_type": "code",
      "outputs": [],
      "source": [
        "def resnet18(split_size, num_boxes, num_classes):\n",
        "    S, B, C = split_size, num_boxes, num_classes\n",
        "\n",
        "    model = torchvision.models.resnet18(pretrained=True)\n",
        "\n",
        "    # Enable backprop for all layers\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = True\n",
        "\n",
        "    # Remove FC layers\n",
        "    modules = list(model.children())[:-2]\n",
        "    model = nn.Sequential(*modules)\n",
        "\n",
        "    # Add custom CNN layer\n",
        "    model.avgpool = nn.AdaptiveAvgPool2d((7, 7))\n",
        "    model = nn.Sequential(model,\n",
        "                          nn.Conv2d(512, 256, kernel_size=(1, 1)))\n",
        "\n",
        "    # Add custom FC layers\n",
        "    model.fc = nn.Sequential(\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(256 * S * S, 1024),\n",
        "        nn.LeakyReLU(0.1),\n",
        "        nn.Dropout(0.5),\n",
        "        nn.Linear(1024, S * S * (C + B * 5)), )\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-03-09T11:53:49.500476Z",
          "start_time": "2024-03-09T11:53:49.484107Z"
        },
        "id": "8141d9639bf30767"
      },
      "id": "8141d9639bf30767",
      "execution_count": 4
    },
    {
      "cell_type": "code",
      "source": [
        "def output_result(out_path, frame_size, frames, fps):\n",
        "    out = cv2.VideoWriter(out_path, cv2.VideoWriter_fourcc(*'DIVX'), fps, frame_size)\n",
        "\n",
        "    for frame in frames:\n",
        "        out.write(frame)\n",
        "\n",
        "    out.release()\n",
        "\n",
        "def convert_cellboxes(predictions, S=7):\n",
        "    \"\"\"\n",
        "    Converts bounding boxes output from Yolo with\n",
        "    an image split size of S into entire image ratios\n",
        "    rather than relative to cell ratios. Tried to do this\n",
        "    vectorized, but this resulted in quite difficult to read\n",
        "    code... Use as a black box? Or implement a more intuitive,\n",
        "    using 2 for loops iterating range(S) and convert them one\n",
        "    by one, resulting in a slower but more readable implementation.\n",
        "    \"\"\"\n",
        "\n",
        "    predictions = predictions.to(\"cpu\")\n",
        "    batch_size = predictions.shape[0]\n",
        "    predictions = predictions.reshape(batch_size, 7, 7, 30)\n",
        "    bboxes1 = predictions[..., 21:25]\n",
        "    bboxes2 = predictions[..., 26:30]\n",
        "    scores = torch.cat(\n",
        "        (predictions[..., 20].unsqueeze(0), predictions[..., 25].unsqueeze(0)), dim=0\n",
        "    )\n",
        "    best_box = scores.argmax(0).unsqueeze(-1)\n",
        "    best_boxes = bboxes1 * (1 - best_box) + best_box * bboxes2\n",
        "    cell_indices = torch.arange(7).repeat(batch_size, 7, 1).unsqueeze(-1)\n",
        "    x = 1 / S * (best_boxes[..., :1] + cell_indices)\n",
        "    y = 1 / S * (best_boxes[..., 1:2] + cell_indices.permute(0, 2, 1, 3))\n",
        "    w_y = 1 / S * best_boxes[..., 2:4]\n",
        "    converted_bboxes = torch.cat((x, y, w_y), dim=-1)\n",
        "    predicted_class = predictions[..., :20].argmax(-1).unsqueeze(-1)\n",
        "    best_confidence = torch.max(predictions[..., 20], predictions[..., 25]).unsqueeze(\n",
        "        -1\n",
        "    )\n",
        "    converted_preds = torch.cat(\n",
        "        (predicted_class, best_confidence, converted_bboxes), dim=-1\n",
        "    )\n",
        "\n",
        "    return converted_preds\n",
        "\n",
        "def cellboxes_to_boxes(out, S=7):\n",
        "    converted_pred = convert_cellboxes(out).reshape(out.shape[0], S * S, -1)\n",
        "    converted_pred[..., 0] = converted_pred[..., 0].long()\n",
        "    all_bboxes = []\n",
        "\n",
        "    for ex_idx in range(out.shape[0]):\n",
        "        bboxes = []\n",
        "\n",
        "        for bbox_idx in range(S * S):\n",
        "            bboxes.append([x.item() for x in converted_pred[ex_idx, bbox_idx, :]])\n",
        "        all_bboxes.append(bboxes)\n",
        "\n",
        "    return all_bboxes\n",
        "\n",
        "def intersection_over_union(boxes_preds, boxes_labels):\n",
        "    box1_x1 = boxes_preds[..., 0:1] - boxes_preds[..., 2:3] / 2\n",
        "    box1_y1 = boxes_preds[..., 1:2] - boxes_preds[..., 3:4] / 2\n",
        "    box1_x2 = boxes_preds[..., 0:1] + boxes_preds[..., 2:3] / 2\n",
        "    box1_y2 = boxes_preds[..., 1:2] + boxes_preds[..., 3:4] / 2\n",
        "    box2_x1 = boxes_labels[..., 0:1] - boxes_labels[..., 2:3] / 2\n",
        "    box2_y1 = boxes_labels[..., 1:2] - boxes_labels[..., 3:4] / 2\n",
        "    box2_x2 = boxes_labels[..., 0:1] + boxes_labels[..., 2:3] / 2\n",
        "    box2_y2 = boxes_labels[..., 1:2] + boxes_labels[..., 3:4] / 2\n",
        "\n",
        "    x1 = torch.max(box1_x1, box2_x1)\n",
        "    y1 = torch.max(box1_y1, box2_y1)\n",
        "    x2 = torch.min(box1_x2, box2_x2)\n",
        "    y2 = torch.min(box1_y2, box2_y2)\n",
        "\n",
        "    # .clamp(0) is for the case when they do not intersect\n",
        "    intersection = (x2 - x1).clamp(0) * (y2 - y1).clamp(0)\n",
        "\n",
        "    box1_area = abs((box1_x2 - box1_x1) * (box1_y2 - box1_y1))\n",
        "    box2_area = abs((box2_x2 - box2_x1) * (box2_y2 - box2_y1))\n",
        "\n",
        "    return intersection / (box1_area + box2_area - intersection + 1e-6)\n",
        "\n",
        "def non_max_suppression(bboxes, iou_threshold, threshold):\n",
        "    bboxes = [box for box in bboxes if box[1] > threshold]\n",
        "    bboxes = sorted(bboxes, key=lambda x: x[1], reverse=True)\n",
        "    bboxes_after_nms = []\n",
        "\n",
        "    while bboxes:\n",
        "        chosen_box = bboxes.pop(0)\n",
        "\n",
        "        bboxes = [\n",
        "            box\n",
        "            for box in bboxes\n",
        "            if box[0] != chosen_box[0]\n",
        "               or intersection_over_union(\n",
        "                torch.tensor(chosen_box[2:]),\n",
        "                torch.tensor(box[2:]))\n",
        "               < iou_threshold\n",
        "        ]\n",
        "\n",
        "        bboxes_after_nms.append(chosen_box)\n",
        "\n",
        "    return bboxes_after_nms\n",
        "\n",
        "def get_batch_bboxes(\n",
        "        x,\n",
        "        model=None,\n",
        "        iou_threshold=0.5,  # for judging True/False\n",
        "        threshold=0.4,  # for NMS\n",
        "        box_format=\"midpoint\",\n",
        "        device=\"cuda\"\n",
        "):\n",
        "    all_boxes = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        model = model.float()\n",
        "        predictions = model(x.float())\n",
        "\n",
        "    batch_size = x.shape[0]\n",
        "    bboxes = cellboxes_to_boxes(predictions)[0]\n",
        "\n",
        "    nms_boxes = non_max_suppression(\n",
        "        bboxes,\n",
        "        iou_threshold=iou_threshold,\n",
        "        threshold=threshold,\n",
        "    )\n",
        "\n",
        "    for nms_box in nms_boxes:\n",
        "      all_boxes.append(nms_box)\n",
        "\n",
        "    return all_boxes, x\n",
        "\n",
        "def draw_bbox_on_image(image, boxes_pred):\n",
        "  height, width, _ = image.shape\n",
        "\n",
        "  for box in boxes_pred:\n",
        "      box_label = box[0:2] # class, confidence\n",
        "      box = box[2:]  # 0:5\n",
        "\n",
        "      prediction_class = INT_TO_LABEL[box_label[0]]\n",
        "\n",
        "      upper_left_x = (box[0] - box[2] / 2) * width\n",
        "      upper_left_y = (box[1] - box[3] / 2) * height\n",
        "\n",
        "      lower_right_x = upper_left_x + (box[2] * width)\n",
        "      lower_right_y = upper_left_y + (box[3] * height)\n",
        "\n",
        "      cv2.rectangle(image, (int(upper_left_x), int(upper_left_y)), (int(lower_right_x), int(lower_right_y)), (0, 255, 0), 2)\n",
        "      image = cv2.putText(image, prediction_class, (int(upper_left_x), int(upper_left_y)), cv2.FONT_HERSHEY_SIMPLEX ,  2, (0, 0, 255) , 1, cv2.LINE_AA)"
      ],
      "metadata": {
        "id": "EVzjmFmbGtaU"
      },
      "id": "EVzjmFmbGtaU",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 127MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "    (4): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (5): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (6): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (7): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
              "  )\n",
              "  (1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "  (fc): Sequential(\n",
              "    (0): Flatten(start_dim=1, end_dim=-1)\n",
              "    (1): Linear(in_features=12544, out_features=1024, bias=True)\n",
              "    (2): LeakyReLU(negative_slope=0.1)\n",
              "    (3): Dropout(p=0.5, inplace=False)\n",
              "    (4): Linear(in_features=1024, out_features=1470, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "checkpoint = torch.load(CHECKPOINT_FILE, map_location=torch.device('cpu'))\n",
        "model = resnet18(split_size=7, num_boxes=2, num_classes=20)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "model.eval()"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-03-09T11:54:00.473378Z",
          "start_time": "2024-03-09T11:54:00.051389Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9667fd8b354d1205",
        "outputId": "0b29ef4a-1b11-4b18-add7-53c8a312686c"
      },
      "id": "9667fd8b354d1205",
      "execution_count": 6
    },
    {
      "cell_type": "code",
      "source": [
        "preprocess = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Resize((IMAGE_HEIGHT, IMAGE_WIDTH)),\n",
        "    transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
        "])"
      ],
      "metadata": {
        "id": "K1tfR4j6Ynvp"
      },
      "id": "K1tfR4j6Ynvp",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-1d690626ed02>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mframe_num\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m   \u001b[0mvideo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCAP_PROP_POS_FRAMES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvideo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "video = cv2.VideoCapture(VIDEO_PATH)\n",
        "frame_count = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "fps = int(video.get(cv2.CAP_PROP_FPS))\n",
        "\n",
        "width  = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "frames = []\n",
        "\n",
        "for frame_num in range(frame_count):\n",
        "  video.set(cv2.CAP_PROP_POS_FRAMES, frame_num)\n",
        "\n",
        "  ret, frame = video.read()\n",
        "\n",
        "  image_for_model = frame.copy()\n",
        "  image_for_model = cv2.cvtColor(image_for_model, cv2.COLOR_BGR2RGB) / 255.0\n",
        "  image_for_model = preprocess(image_for_model)\n",
        "  image_for_model = image_for_model[None, :, :, :]\n",
        "\n",
        "  pred_bboxes, image = get_batch_bboxes(\n",
        "      x=image_for_model,\n",
        "      model=model,\n",
        "      iou_threshold=0.5,\n",
        "      threshold=0.4,\n",
        "      device=DEVICE\n",
        "  )\n",
        "\n",
        "  draw_bbox_on_image(frame, pred_bboxes)\n",
        "\n",
        "  frames.append(frame)\n",
        "\n",
        "output_result(RESULT_VIDEO_PATH, (width, height), frames, fps)"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-03-09T13:14:41.614289Z",
          "start_time": "2024-03-09T13:12:18.551562Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "fcd963832df93e94",
        "outputId": "4874a54a-153f-466d-eff1-d46096b6f58f"
      },
      "id": "fcd963832df93e94",
      "execution_count": 8
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}